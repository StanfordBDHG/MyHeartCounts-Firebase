{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vc25e3OK83Ly"
   },
   "source": "# LLM Prompting Research Tool - Simplified\n\nThis notebook tests English and Spanish prompts with configurable step count and education level for MyHeartCounts nudge generation.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jqTnkzaM83L2"
   },
   "outputs": [],
   "source": "import json\nimport requests\nfrom google.colab import userdata\nfrom typing import Dict, Any"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nITJ6O3-83L3"
   },
   "source": "## Configuration\n\n**Edit these values to test different scenarios:**",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-nXCjvaW83L3",
    "outputId": "12a112fb-4c96-453b-de9a-d44d39047f6b"
   },
   "outputs": [],
   "source": "# USER CONFIGURATION - Edit these values\nSTEP_COUNT = 5000                    # Daily step count to include in prompts\nEDUCATION_LEVEL = \"Bachelor's degree\" # Education level to include in prompts\nLANGUAGE = 'en'                      # 'en' for English, 'es' for Spanish\n\n# OpenAI Configuration\nOPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\nMODEL = 'gpt-3.5-turbo'\nMAX_TOKENS = 1000\nTEMPERATURE = 0.7\n\nprint(f\"‚úÖ Configuration set: {STEP_COUNT} steps, education: {EDUCATION_LEVEL}, language: {LANGUAGE}\")\nprint(\"‚úÖ API key configured\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKP3JAcI83L3"
   },
   "source": "## LLM Interface",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BvO7zg3K83L3"
   },
   "outputs": [],
   "source": "class LLMTester:\n    def __init__(self, api_key: str, model: str = MODEL, max_tokens: int = MAX_TOKENS, temperature: float = TEMPERATURE):\n        self.api_key = api_key\n        self.model = model\n        self.max_tokens = max_tokens\n        self.temperature = temperature\n\n    async def call_openai_api(self, prompt: str) -> Dict[str, Any]:\n        try:\n            response = requests.post(\n                'https://api.openai.com/v1/chat/completions',\n                headers={\n                    'Authorization': f'Bearer {self.api_key}',\n                    'Content-Type': 'application/json'\n                },\n                json={\n                    'model': self.model,\n                    'messages': [{'role': 'user', 'content': prompt}],\n                    'max_tokens': self.max_tokens,\n                    'temperature': self.temperature\n                },\n                timeout=30\n            )\n\n            if not response.ok:\n                raise Exception(f'OpenAI API error: {response.status_code} {response.text}')\n\n            data = response.json()\n            content = data['choices'][0]['message']['content']\n            return {'success': True, 'content': content, 'error': None}\n\n        except Exception as error:\n            return {'success': False, 'content': None, 'error': str(error)}\n\n    def validate_response(self, content: str) -> Dict[str, Any]:\n        \"\"\"Validate LLM response format\"\"\"\n        try:\n            parsed_nudges = json.loads(content)\n            \n            if not isinstance(parsed_nudges, list):\n                return {'is_valid': False, 'error': 'Response is not an array'}\n            \n            if len(parsed_nudges) != 7:\n                return {'is_valid': False, 'error': f'Expected 7 items, got {len(parsed_nudges)}'}\n            \n            for i, nudge in enumerate(parsed_nudges):\n                if not isinstance(nudge, dict):\n                    return {'is_valid': False, 'error': f'Item {i} is not an object'}\n                if 'title' not in nudge or 'body' not in nudge:\n                    return {'is_valid': False, 'error': f'Item {i} missing title or body'}\n                if not nudge['title'].strip() or not nudge['body'].strip():\n                    return {'is_valid': False, 'error': f'Item {i} has empty title or body'}\n            \n            return {'is_valid': True, 'parsed_data': parsed_nudges, 'error': None}\n            \n        except json.JSONDecodeError as e:\n            return {'is_valid': False, 'error': f'Invalid JSON: {str(e)}'}\n\n    async def test_prompt(self, prompt: str) -> Dict[str, Any]:\n        \"\"\"Test a prompt and return results\"\"\"\n        print(f\"üß™ Testing prompt...\")\n        print(f\"üìù Prompt length: {len(prompt)} characters\")\n        \n        # Call API\n        api_result = await self.call_openai_api(prompt)\n        \n        if api_result['success']:\n            print(\"‚úÖ API call successful\")\n            \n            # Validate response\n            validation = self.validate_response(api_result['content'])\n            \n            if validation['is_valid']:\n                print(\"‚úÖ Response validation passed\")\n                return {\n                    'success': True,\n                    'nudges': validation['parsed_data'],\n                    'raw_response': api_result['content']\n                }\n            else:\n                print(f\"‚ùå Response validation failed: {validation['error']}\")\n                return {\n                    'success': False,\n                    'error': validation['error'],\n                    'raw_response': api_result['content']\n                }\n        else:\n            print(f\"‚ùå API call failed: {api_result['error']}\")\n            return {\n                'success': False,\n                'error': api_result['error'],\n                'raw_response': None\n            }\n\n# Initialize tester\ntester = LLMTester(OPENAI_API_KEY)\nprint(\"üöÄ LLM Tester initialized\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eistMGqg83L4"
   },
   "source": "## Prompts with Step Count and Education Level\n\n**Edit these prompts to test different approaches:**",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7dqpu8VU83L4"
   },
   "outputs": [],
   "source": "def get_english_prompt(step_count: int, education_level: str) -> str:\n    \"\"\"Edit this English prompt to test different approaches\"\"\"\n    return f\"\"\"Generate 7 motivational sports and exercise nudges for a heart health study participant.\n\nParticipant information:\n- Recent daily step count average: {step_count}\n- Education level: {education_level}\n\nEach nudge should:\n- Be encouraging and positive\n- Focus on different types of physical activities and sports\n- Be personalized and engaging based on the participant's step count and education level\n- Include a clear call to action\n- Be suitable for someone in a heart health study\n- Incorporate step count references when relevant\n- Adapt language and suggestions to the participant's education level\n\nReturn the response as a JSON array with exactly 7 objects, each having \"title\" and \"body\" fields.\nExample format:\n[\n  {{\"title\": \"Morning Energy Boost\", \"body\": \"Start your day with a 15-minute walk! Your heart will love the gentle cardio.\"}},\n  ...\n]\n\nMake each nudge unique and focus on different activities like walking, swimming, dancing, team sports, strength training, yoga, etc.\"\"\"\n\ndef get_spanish_prompt(step_count: int, education_level: str) -> str:\n    \"\"\"Edit this Spanish prompt to test different approaches\"\"\"\n    return f\"\"\"Genera 7 recordatorios motivacionales de deportes y ejercicio para un participante en un estudio de salud card√≠aca.\n\nInformaci√≥n del participante:\n- Promedio de pasos diarios recientes: {step_count}\n- Nivel educativo: {education_level}\n\nCada recordatorio debe:\n- Ser alentador y positivo\n- Enfocarse en diferentes tipos de actividades f√≠sicas y deportes\n- Ser personalizado y atractivo basado en el conteo de pasos y nivel educativo del participante\n- Incluir una llamada clara a la acci√≥n\n- Ser adecuado para alguien en un estudio de salud card√≠aca\n- Incorporar referencias al conteo de pasos cuando sea relevante\n- Adaptar el lenguaje y las sugerencias al nivel educativo del participante\n\nDevuelve la respuesta como un array JSON con exactamente 7 objetos, cada uno con campos \"title\" y \"body\".\nFormato de ejemplo:\n[\n  {{\"title\": \"Impulso de Energ√≠a Matutino\", \"body\": \"¬°Comienza tu d√≠a con una caminata de 15 minutos! Tu coraz√≥n amar√° el cardio suave.\"}},\n  ...\n]\n\nHaz cada recordatorio √∫nico y enf√≥cate en diferentes actividades como caminar, nadar, bailar, deportes de equipo, entrenamiento de fuerza, yoga, etc.\"\"\"\n\nprint(\"üìù Prompt functions defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vi2ndgo_83L4"
   },
   "source": "## Run Test\n\n**Execute this cell to test your configured prompt:**",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pixIaqy983L4",
    "outputId": "a487f4d3-44ec-4be9-c561-07c54f836ef7"
   },
   "outputs": [],
   "source": "# Generate prompt based on configuration\nif LANGUAGE == 'en':\n    prompt = get_english_prompt(STEP_COUNT, EDUCATION_LEVEL)\n    print(f\"üá∫üá∏ Testing English prompt with {STEP_COUNT} steps, education: {EDUCATION_LEVEL}\")\nelif LANGUAGE == 'es':\n    prompt = get_spanish_prompt(STEP_COUNT, EDUCATION_LEVEL)\n    print(f\"üá™üá∏ Testing Spanish prompt with {STEP_COUNT} steps, education: {EDUCATION_LEVEL}\")\nelse:\n    print(\"‚ùå Invalid language. Use 'en' or 'es'\")\n    prompt = None\n\nif prompt:\n    # Print full prompt for debugging\n    print(\"\\n\" + \"=\"*80)\n    print(\"üì§ FULL PROMPT SENT TO LLM:\")\n    print(\"=\"*80)\n    print(prompt)\n    print(\"=\"*80)\n    \n    # Test the prompt\n    result = await tester.test_prompt(prompt)\n    \n    # Print full LLM response for debugging\n    print(\"\\n\" + \"=\"*80)\n    print(\"üì• FULL LLM RESPONSE:\")\n    print(\"=\"*80)\n    if result['raw_response']:\n        print(result['raw_response'])\n    else:\n        print(\"No response received\")\n    print(\"=\"*80)\n    \n    # Display results\n    if result['success']:\n        print(\"\\nüìã Generated Nudges:\")\n        for i, nudge in enumerate(result['nudges'], 1):\n            print(f\"{i}. {nudge['title']}\")\n            print(f\"   {nudge['body']}\\n\")\n    else:\n        print(f\"\\n‚ùå Test failed: {result['error']}\")\n        print(\"Check the full LLM response above for details.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}