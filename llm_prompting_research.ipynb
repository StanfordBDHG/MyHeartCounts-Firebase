{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc25e3OK83Ly"
      },
      "source": [
        "# LLM Prompting Research Tool\n",
        "\n",
        "This is a small notebook to test out the LLM Prompting for MHC. Written on 06-20-25."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqTnkzaM83L2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nITJ6O3-83L3"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "Set up the OpenAI API key and model configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nXCjvaW83L3",
        "outputId": "12a112fb-4c96-453b-de9a-d44d39047f6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ API key configured\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "MODEL = 'gpt-3.5-turbo'\n",
        "MAX_TOKENS = 1000\n",
        "TEMPERATURE = 0.7\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "print(\"‚úÖ API key configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKP3JAcI83L3"
      },
      "source": [
        "## Core LLM Interface\n",
        "\n",
        "Taken from the planNudges.ts implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvO7zg3K83L3"
      },
      "outputs": [],
      "source": [
        "class LLMPromptTester:\n",
        "    def __init__(self, api_key: str, model: str = MODEL, max_tokens: int = MAX_TOKENS, temperature: float = TEMPERATURE):\n",
        "        self.api_key = api_key\n",
        "        self.model = model\n",
        "        self.max_tokens = max_tokens\n",
        "        self.temperature = temperature\n",
        "        self.results_history = []\n",
        "\n",
        "    async def call_openai_api(self, prompt: str, retries: int = MAX_RETRIES) -> Dict[str, Any]:\n",
        "        last_error = None\n",
        "\n",
        "        for attempt in range(1, retries + 1):\n",
        "            try:\n",
        "                response = requests.post(\n",
        "                    'https://api.openai.com/v1/chat/completions',\n",
        "                    headers={\n",
        "                        'Authorization': f'Bearer {self.api_key}',\n",
        "                        'Content-Type': 'application/json'\n",
        "                    },\n",
        "                    json={\n",
        "                        'model': self.model,\n",
        "                        'messages': [{'role': 'user', 'content': prompt}],\n",
        "                        'max_tokens': self.max_tokens,\n",
        "                        'temperature': self.temperature\n",
        "                    },\n",
        "                    timeout=30\n",
        "                )\n",
        "\n",
        "                if not response.ok:\n",
        "                    raise Exception(f'OpenAI API error: {response.status_code} {response.text}')\n",
        "\n",
        "                data = response.json()\n",
        "                content = data['choices'][0]['message']['content']\n",
        "\n",
        "                return {\n",
        "                    'success': True,\n",
        "                    'content': content,\n",
        "                    'attempt': attempt,\n",
        "                    'error': None\n",
        "                }\n",
        "\n",
        "            except Exception as error:\n",
        "                last_error = str(error)\n",
        "                print(f\"Attempt {attempt}/{retries} failed: {error}\")\n",
        "\n",
        "                if attempt < retries:\n",
        "                    import time\n",
        "                    time.sleep(attempt)  # Progressive backoff\n",
        "\n",
        "        return {\n",
        "            'success': False,\n",
        "            'content': None,\n",
        "            'attempt': retries,\n",
        "            'error': last_error\n",
        "        }\n",
        "\n",
        "    def validate_nudge_response(self, content: str, expected_count: int = 7) -> Dict[str, Any]:\n",
        "        \"\"\"Validate LLM response format based on planNudges.ts validation logic\"\"\"\n",
        "        validation_result = {\n",
        "            'is_valid': False,\n",
        "            'parsed_data': None,\n",
        "            'errors': [],\n",
        "            'warnings': []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Try to parse JSON\n",
        "            parsed_nudges = json.loads(content)\n",
        "            validation_result['parsed_data'] = parsed_nudges\n",
        "\n",
        "            # Check if it's an array\n",
        "            if not isinstance(parsed_nudges, list):\n",
        "                validation_result['errors'].append('Response is not an array')\n",
        "                return validation_result\n",
        "\n",
        "            # Check expected count\n",
        "            if len(parsed_nudges) != expected_count:\n",
        "                validation_result['errors'].append(f'Expected {expected_count} items, got {len(parsed_nudges)}')\n",
        "\n",
        "            # Validate each nudge structure\n",
        "            for i, nudge in enumerate(parsed_nudges):\n",
        "                if not isinstance(nudge, dict):\n",
        "                    validation_result['errors'].append(f'Item {i} is not an object')\n",
        "                    continue\n",
        "\n",
        "                if 'title' not in nudge:\n",
        "                    validation_result['errors'].append(f'Item {i} missing \"title\" field')\n",
        "                elif not isinstance(nudge['title'], str) or not nudge['title'].strip():\n",
        "                    validation_result['errors'].append(f'Item {i} has invalid title')\n",
        "\n",
        "                if 'body' not in nudge:\n",
        "                    validation_result['errors'].append(f'Item {i} missing \"body\" field')\n",
        "                elif not isinstance(nudge['body'], str) or not nudge['body'].strip():\n",
        "                    validation_result['errors'].append(f'Item {i} has invalid body')\n",
        "\n",
        "                # Check for reasonable length\n",
        "                if 'title' in nudge and len(nudge['title']) > 100:\n",
        "                    validation_result['warnings'].append(f'Item {i} title is very long ({len(nudge[\"title\"])} chars)')\n",
        "\n",
        "                if 'body' in nudge and len(nudge['body']) > 300:\n",
        "                    validation_result['warnings'].append(f'Item {i} body is very long ({len(nudge[\"body\"])} chars)')\n",
        "\n",
        "            # If no errors, mark as valid\n",
        "            if not validation_result['errors']:\n",
        "                validation_result['is_valid'] = True\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            validation_result['errors'].append(f'Invalid JSON: {str(e)}')\n",
        "        except Exception as e:\n",
        "            validation_result['errors'].append(f'Validation error: {str(e)}')\n",
        "\n",
        "        return validation_result\n",
        "\n",
        "    async def test_prompt(self, prompt: str, expected_count: int = 7, description: str = \"\") -> Dict[str, Any]:\n",
        "        \"\"\"Test a prompt and validate the response\"\"\"\n",
        "        print(f\"\\nüß™ Testing prompt: {description or 'Unnamed test'}\")\n",
        "        print(f\"üìù Prompt length: {len(prompt)} characters\")\n",
        "\n",
        "        # Call API\n",
        "        api_result = await self.call_openai_api(prompt)\n",
        "\n",
        "        result = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'description': description,\n",
        "            'prompt': prompt,\n",
        "            'api_success': api_result['success'],\n",
        "            'api_attempts': api_result['attempt'],\n",
        "            'api_error': api_result['error'],\n",
        "            'raw_response': api_result['content'],\n",
        "            'validation': None\n",
        "        }\n",
        "\n",
        "        if api_result['success']:\n",
        "            print(f\"‚úÖ API call successful (attempt {api_result['attempt']})\")\n",
        "\n",
        "            # Validate response\n",
        "            validation = self.validate_nudge_response(api_result['content'], expected_count)\n",
        "            result['validation'] = validation\n",
        "\n",
        "            if validation['is_valid']:\n",
        "                print(f\"‚úÖ Response validation passed\")\n",
        "                if validation['warnings']:\n",
        "                    print(f\"‚ö†Ô∏è  Warnings: {len(validation['warnings'])}\")\n",
        "                    for warning in validation['warnings']:\n",
        "                        print(f\"   - {warning}\")\n",
        "            else:\n",
        "                print(f\"‚ùå Response validation failed\")\n",
        "                print(f\"üîç Errors: {len(validation['errors'])}\")\n",
        "                for error in validation['errors']:\n",
        "                    print(f\"   - {error}\")\n",
        "        else:\n",
        "            print(f\"‚ùå API call failed after {api_result['attempt']} attempts\")\n",
        "            print(f\"üîç Error: {api_result['error']}\")\n",
        "\n",
        "        # Store result\n",
        "        self.results_history.append(result)\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eistMGqg83L4"
      },
      "source": [
        "## Predefined Test Prompts\n",
        "\n",
        "Based on the nudge generation prompts from planNudges.ts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dqpu8VU83L4"
      },
      "outputs": [],
      "source": [
        "# Original English prompt from planNudges.ts\n",
        "\n",
        "ORIGINAL_ENGLISH_PROMPT = \"\"\"Generate 7 motivational sports and exercise nudges for a heart health study participant. Each nudge should:\n",
        "- Be encouraging and positive\n",
        "- Focus on different types of physical activities and sports\n",
        "- Be personalized and engaging\n",
        "- Include a clear call to action\n",
        "- Be suitable for someone in a heart health study\n",
        "- Optionally include dynamic data variables (e.g., {{step_count}}, {{weekly_goal}}, {{last_activity}}, {{weather}}, etc.) that can be filled in later\n",
        "\n",
        "Return the response as a JSON array with exactly 7 objects, each having \"title\" and \"body\" fields.\n",
        "Example format:\n",
        "[\n",
        "  {\"title\": \"Morning Energy Boost\", \"body\": \"Start your day with a 15-minute walk! Your heart will love the gentle cardio.\"},\n",
        "  {\"title\": \"Step It Up\", \"body\": \"You're just {{step_count}} steps away from your goal‚Äîkeep moving!\"}\n",
        "]\n",
        "\n",
        "Make each nudge unique and focus on different activities like walking, swimming, dancing, team sports, strength training, yoga, etc.\"\"\"\n",
        "\n",
        "# Original Spanish prompt from planNudges.ts\n",
        "ORIGINAL_SPANISH_PROMPT = \"\"\"Genera 7 recordatorios motivacionales de deportes y ejercicio para un participante en un estudio de salud card√≠aca. Cada recordatorio debe:\n",
        "- Ser alentador y positivo\n",
        "- Enfocarse en diferentes tipos de actividades f√≠sicas y deportes\n",
        "- Ser personalizado y atractivo\n",
        "- Incluir una llamada clara a la acci√≥n\n",
        "- Ser adecuado para alguien en un estudio de salud card√≠aca\n",
        "\n",
        "Devuelve la respuesta como un array JSON con exactamente 7 objetos, cada uno con campos \"title\" y \"body\".\n",
        "Formato de ejemplo:\n",
        "[\n",
        "  {\"title\": \"Impulso de Energ√≠a Matutino\", \"body\": \"¬°Comienza tu d√≠a con una caminata de 15 minutos! Tu coraz√≥n amar√° el cardio suave.\"},\n",
        "  ...\n",
        "]\n",
        "\n",
        "Haz cada recordatorio √∫nico y enf√≥cate en diferentes actividades como caminar, nadar, bailar, deportes de equipo, entrenamiento de fuerza, yoga, etc.\"\"\"\n",
        "\n",
        "# Test prompts for experimentation\n",
        "TEST_PROMPTS = {\n",
        "    'original_english': ORIGINAL_ENGLISH_PROMPT,\n",
        "    'original_spanish': ORIGINAL_SPANISH_PROMPT,\n",
        "\n",
        "    'simplified_english': \"\"\"Create 7 exercise reminders for heart health study participants. Each should have a \"title\" and \"body\". Return as JSON array.\n",
        "Make them motivational and focus on different activities.\"\"\",\n",
        "\n",
        "    'detailed_english': \"\"\"Generate exactly 7 motivational exercise nudges for heart health study participants. Requirements:\n",
        "1. Each nudge must be encouraging and positive\n",
        "2. Focus on diverse physical activities (walking, swimming, cycling, dancing, strength training, yoga, team sports)\n",
        "3. Include specific, actionable advice\n",
        "4. Appropriate for cardiovascular health improvement\n",
        "5. Vary the tone and approach for each nudge\n",
        "\n",
        "CRITICAL: Return ONLY a valid JSON array with exactly 7 objects. Each object must have exactly these fields:\n",
        "- \"title\": string (maximum 50 characters)\n",
        "- \"body\": string (maximum 150 characters)\n",
        "\n",
        "Example format:\n",
        "[{\"title\": \"Morning Walk Challenge\", \"body\": \"Start today with a 10-minute walk. Your heart will thank you!\"}]\"\"\",\n",
        "\n",
        "    'json_focused': \"\"\"You are a JSON generator. Generate exactly 7 exercise nudges.\n",
        "\n",
        "Output ONLY valid JSON in this exact format:\n",
        "[{\"title\": \"Exercise Title\", \"body\": \"Exercise description with call to action\"}]\n",
        "\n",
        "Requirements:\n",
        "- Exactly 7 items\n",
        "- Heart health focus\n",
        "- Different activities each\n",
        "- Positive tone\n",
        "- No additional text outside JSON\"\"\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi2ndgo_83L4"
      },
      "source": [
        "## Initialize Tester and Run Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pixIaqy983L4",
        "outputId": "a487f4d3-44ec-4be9-c561-07c54f836ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ LLM Prompt Tester initialized\n",
            "üìä Model: gpt-3.5-turbo\n",
            "üå°Ô∏è  Temperature: 0.7\n",
            "üìù Max tokens: 1000\n"
          ]
        }
      ],
      "source": [
        "# Initialize the tester\n",
        "tester = LLMPromptTester(OPENAI_API_KEY)\n",
        "\n",
        "print(\"üöÄ LLM Prompt Tester initialized\")\n",
        "print(f\"üìä Model: {MODEL}\")\n",
        "print(f\"üå°Ô∏è  Temperature: {TEMPERATURE}\")\n",
        "print(f\"üìù Max tokens: {MAX_TOKENS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG9YdIv_83L4"
      },
      "source": [
        "## Test Individual Prompts\n",
        "\n",
        "Run this cell to test a specific prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "ZJUl6Ih983L4",
        "outputId": "9caf5783-46f2-4e33-d127-fd675af7cd6f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'TEST_PROMPTS' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-14-3241922099.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprompt_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ORIGINAL_ENGLISH_PROMPT'\u001b[0m  \u001b[0;31m# Change this to test different prompts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m result = await tester.test_prompt(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mTEST_PROMPTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Testing {prompt_name} prompt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TEST_PROMPTS' is not defined"
          ]
        }
      ],
      "source": [
        "# Test a specific prompt\n",
        "prompt_name = 'ORIGINAL_ENGLISH_PROMPT'  # Change this to test different prompts\n",
        "result = await tester.test_prompt(\n",
        "    TEST_PROMPTS[prompt_name],\n",
        "    description=f\"Testing {prompt_name} prompt\"\n",
        ")\n",
        "\n",
        "# Display the result\n",
        "if result['api_success'] and result['validation']['is_valid']:\n",
        "    print(\"\\nüìã Generated Nudges:\")\n",
        "    for i, nudge in enumerate(result['validation']['parsed_data'], 1):\n",
        "        print(f\"{i}. {nudge['title']}\")\n",
        "        print(f\"   {nudge['body']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm32i5Wj83L4"
      },
      "source": [
        "## Batch Test All Prompts\n",
        "\n",
        "Run all test prompts and compare results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhOKM7WR83L4",
        "outputId": "6fc70657-714d-43e4-f082-7ed70e083e61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî¨ Running batch tests on all prompts...\n",
            "\n",
            "\n",
            "üß™ Testing prompt: Batch test: original_english\n",
            "üìù Prompt length: 698 characters\n",
            "‚úÖ API call successful (attempt 1)\n",
            "‚úÖ Response validation passed\n",
            "\n",
            "üß™ Testing prompt: Batch test: original_spanish\n",
            "üìù Prompt length: 790 characters\n",
            "‚úÖ API call successful (attempt 1)\n",
            "‚úÖ Response validation passed\n",
            "\n",
            "üß™ Testing prompt: Batch test: simplified_english\n",
            "üìù Prompt length: 183 characters\n",
            "‚úÖ API call successful (attempt 1)\n",
            "‚úÖ Response validation passed\n",
            "\n",
            "üß™ Testing prompt: Batch test: detailed_english\n",
            "üìù Prompt length: 720 characters\n",
            "‚úÖ API call successful (attempt 1)\n",
            "‚úÖ Response validation passed\n",
            "\n",
            "üß™ Testing prompt: Batch test: json_focused\n",
            "üìù Prompt length: 321 characters\n",
            "‚úÖ API call successful (attempt 1)\n",
            "‚úÖ Response validation passed\n",
            "\n",
            "‚úÖ Batch testing complete!\n"
          ]
        }
      ],
      "source": [
        "# Test all prompts\n",
        "print(\"üî¨ Running batch tests on all prompts...\\n\")\n",
        "\n",
        "batch_results = {}\n",
        "for prompt_name, prompt_text in TEST_PROMPTS.items():\n",
        "    result = await tester.test_prompt(\n",
        "        prompt_text,\n",
        "        description=f\"Batch test: {prompt_name}\"\n",
        "    )\n",
        "    batch_results[prompt_name] = result\n",
        "\n",
        "    # Small delay to avoid rate limiting\n",
        "    import time\n",
        "    time.sleep(1)\n",
        "\n",
        "print(\"\\n‚úÖ Batch testing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WyvV5k783L5"
      },
      "source": [
        "## Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIhkDSOF83L5",
        "outputId": "3479a3fc-13ca-4f6d-f21f-ddadd7e2654a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Results Summary:\n",
            "       prompt_name  api_success  api_attempts  validation_success  error_count  warning_count  prompt_length\n",
            "  original_english         True             1                True            0              0            698\n",
            "  original_spanish         True             1                True            0              0            790\n",
            "simplified_english         True             1                True            0              0            183\n",
            "  detailed_english         True             1                True            0              0            720\n",
            "      json_focused         True             1                True            0              0            321\n",
            "\n",
            "üéØ Overall success rate: 100.0%\n"
          ]
        }
      ],
      "source": [
        "# Analyze results\n",
        "results_summary = []\n",
        "\n",
        "for prompt_name, result in batch_results.items():\n",
        "    summary = {\n",
        "        'prompt_name': prompt_name,\n",
        "        'api_success': result['api_success'],\n",
        "        'api_attempts': result['api_attempts'],\n",
        "        'validation_success': result['validation']['is_valid'] if result['validation'] else False,\n",
        "        'error_count': len(result['validation']['errors']) if result['validation'] else 0,\n",
        "        'warning_count': len(result['validation']['warnings']) if result['validation'] else 0,\n",
        "        'prompt_length': len(result['prompt'])\n",
        "    }\n",
        "    results_summary.append(summary)\n",
        "\n",
        "# Create results DataFrame\n",
        "df_results = pd.DataFrame(results_summary)\n",
        "print(\"üìä Results Summary:\")\n",
        "print(df_results.to_string(index=False))\n",
        "\n",
        "# Success rate\n",
        "success_rate = (df_results['validation_success'].sum() / len(df_results)) * 100\n",
        "print(f\"\\nüéØ Overall success rate: {success_rate:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJLGJ9iE83L5"
      },
      "source": [
        "## Custom Prompt Testing\n",
        "\n",
        "Use this section to test your own custom prompts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0eqwhpF83L5",
        "outputId": "a8248fc3-a372-4857-a23b-0723d925bd5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Testing prompt: Custom prompt test\n",
            "üìù Prompt length: 34 characters\n",
            "‚úÖ API call successful (attempt 1)\n",
            "‚ùå Response validation failed\n",
            "üîç Errors: 1\n",
            "   - Invalid JSON: Expecting value: line 1 column 1 (char 0)\n"
          ]
        }
      ],
      "source": [
        "# Custom prompt for testing\n",
        "custom_prompt = \"\"\"\n",
        "Write your custom prompt here...\n",
        "\"\"\"\n",
        "\n",
        "# Test custom prompt\n",
        "if custom_prompt.strip():\n",
        "    custom_result = await tester.test_prompt(\n",
        "        custom_prompt,\n",
        "        description=\"Custom prompt test\"\n",
        "    )\n",
        "\n",
        "    if custom_result['api_success'] and custom_result['validation']['is_valid']:\n",
        "        print(\"\\nüìã Custom Prompt Results:\")\n",
        "        for i, nudge in enumerate(custom_result['validation']['parsed_data'], 1):\n",
        "            print(f\"{i}. {nudge['title']}\")\n",
        "            print(f\"   {nudge['body']}\\n\")\n",
        "else:\n",
        "    print(\"‚úèÔ∏è  Enter a custom prompt in the cell above to test it\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ1Kc5bY83L5"
      },
      "source": [
        "## Export Results\n",
        "\n",
        "Export test results for further analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65TpovEp83L5"
      },
      "outputs": [],
      "source": [
        "# Export results to JSON\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "export_data = {\n",
        "    'test_session': {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'model': MODEL,\n",
        "        'temperature': TEMPERATURE,\n",
        "        'max_tokens': MAX_TOKENS\n",
        "    },\n",
        "    'results': tester.results_history\n",
        "}\n",
        "\n",
        "filename = f\"llm_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "with open(filename, 'w') as f:\n",
        "    json.dump(export_data, f, indent=2)\n",
        "\n",
        "print(f\"üìÅ Results exported to: {filename}\")\n",
        "print(f\"üìä Total tests conducted: {len(tester.results_history)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVUCSMkO83L5"
      },
      "source": [
        "## Validation Utilities\n",
        "\n",
        "Additional utilities for response validation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sj4zqLsE83L5"
      },
      "outputs": [],
      "source": [
        "def analyze_response_quality(responses: List[Dict]) -> Dict[str, Any]:\n",
        "    \"\"\"Analyze the quality and characteristics of generated responses\"\"\"\n",
        "    analysis = {\n",
        "        'title_lengths': [],\n",
        "        'body_lengths': [],\n",
        "        'common_words': {},\n",
        "        'activity_types': [],\n",
        "        'sentiment_indicators': []\n",
        "    }\n",
        "\n",
        "    for response in responses:\n",
        "        if 'title' in response:\n",
        "            analysis['title_lengths'].append(len(response['title']))\n",
        "\n",
        "        if 'body' in response:\n",
        "            analysis['body_lengths'].append(len(response['body']))\n",
        "\n",
        "            # Simple word frequency analysis\n",
        "            words = re.findall(r'\\b\\w+\\b', response['body'].lower())\n",
        "            for word in words:\n",
        "                if len(word) > 3:  # Skip short words\n",
        "                    analysis['common_words'][word] = analysis['common_words'].get(word, 0) + 1\n",
        "\n",
        "    # Calculate statistics\n",
        "    if analysis['title_lengths']:\n",
        "        analysis['avg_title_length'] = sum(analysis['title_lengths']) / len(analysis['title_lengths'])\n",
        "    if analysis['body_lengths']:\n",
        "        analysis['avg_body_length'] = sum(analysis['body_lengths']) / len(analysis['body_lengths'])\n",
        "\n",
        "    # Top common words\n",
        "    analysis['top_words'] = sorted(analysis['common_words'].items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "    return analysis\n",
        "\n",
        "# Example usage with successful results\n",
        "successful_responses = []\n",
        "for result in tester.results_history:\n",
        "    if result['api_success'] and result['validation'] and result['validation']['is_valid']:\n",
        "        successful_responses.extend(result['validation']['parsed_data'])\n",
        "\n",
        "if successful_responses:\n",
        "    quality_analysis = analyze_response_quality(successful_responses)\n",
        "    print(\"üìà Response Quality Analysis:\")\n",
        "    print(f\"Average title length: {quality_analysis.get('avg_title_length', 0):.1f} characters\")\n",
        "    print(f\"Average body length: {quality_analysis.get('avg_body_length', 0):.1f} characters\")\n",
        "    print(f\"Most common words: {[word for word, count in quality_analysis['top_words'][:5]]}\")\n",
        "else:\n",
        "    print(\"No successful responses to analyze yet. Run some tests first!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}