{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vc25e3OK83Ly"
   },
   "source": [
    "# LLM Prompting Research Tool\n",
    "\n",
    "This is a small notebook to test out the LLM Prompting for MHC. Written on 06-20-25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqTnkzaM83L2"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "from google.colab import userdata\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nITJ6O3-83L3"
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the OpenAI API key and model configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-nXCjvaW83L3",
    "outputId": "12a112fb-4c96-453b-de9a-d44d39047f6b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ API key configured\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "MODEL = 'gpt-3.5-turbo'\n",
    "MAX_TOKENS = 1000\n",
    "TEMPERATURE = 0.7\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "print(\"‚úÖ API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKP3JAcI83L3"
   },
   "source": [
    "## Core LLM Interface\n",
    "\n",
    "Taken from the planNudges.ts implementation:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BvO7zg3K83L3"
   },
   "outputs": [],
   "source": "class LLMPromptTester:\n    def __init__(self, api_key: str, model: str = MODEL, max_tokens: int = MAX_TOKENS, temperature: float = TEMPERATURE):\n        self.api_key = api_key\n        self.model = model\n        self.max_tokens = max_tokens\n        self.temperature = temperature\n        self.results_history = []\n\n    async def call_openai_api(self, prompt: str, retries: int = MAX_RETRIES) -> Dict[str, Any]:\n        last_error = None\n\n        for attempt in range(1, retries + 1):\n            try:\n                response = requests.post(\n                    'https://api.openai.com/v1/chat/completions',\n                    headers={\n                        'Authorization': f'Bearer {self.api_key}',\n                        'Content-Type': 'application/json'\n                    },\n                    json={\n                        'model': self.model,\n                        'messages': [{'role': 'user', 'content': prompt}],\n                        'max_tokens': self.max_tokens,\n                        'temperature': self.temperature\n                    },\n                    timeout=30\n                )\n\n                if not response.ok:\n                    raise Exception(f'OpenAI API error: {response.status_code} {response.text}')\n\n                data = response.json()\n                content = data['choices'][0]['message']['content']\n\n                return {\n                    'success': True,\n                    'content': content,\n                    'attempt': attempt,\n                    'error': None\n                }\n\n            except Exception as error:\n                last_error = str(error)\n                print(f\"Attempt {attempt}/{retries} failed: {error}\")\n\n                if attempt < retries:\n                    import time\n                    time.sleep(attempt)  # Progressive backoff\n\n        return {\n            'success': False,\n            'content': None,\n            'attempt': retries,\n            'error': last_error\n        }\n\n    def validate_nudge_response(self, content: str, expected_count: int = 7) -> Dict[str, Any]:\n        \"\"\"Validate LLM response format based on planNudges.ts validation logic\"\"\"\n        validation_result = {\n            'is_valid': False,\n            'parsed_data': None,\n            'errors': [],\n            'warnings': []\n        }\n\n        try:\n            # Try to parse JSON\n            parsed_nudges = json.loads(content)\n            validation_result['parsed_data'] = parsed_nudges\n\n            # Check if it's an array\n            if not isinstance(parsed_nudges, list):\n                validation_result['errors'].append('Response is not an array')\n                return validation_result\n\n            # Check expected count\n            if len(parsed_nudges) != expected_count:\n                validation_result['errors'].append(f'Expected {expected_count} items, got {len(parsed_nudges)}')\n\n            # Validate each nudge structure\n            for i, nudge in enumerate(parsed_nudges):\n                if not isinstance(nudge, dict):\n                    validation_result['errors'].append(f'Item {i} is not an object')\n                    continue\n\n                if 'title' not in nudge:\n                    validation_result['errors'].append(f'Item {i} missing \"title\" field')\n                elif not isinstance(nudge['title'], str) or not nudge['title'].strip():\n                    validation_result['errors'].append(f'Item {i} has invalid title')\n\n                if 'body' not in nudge:\n                    validation_result['errors'].append(f'Item {i} missing \"body\" field')\n                elif not isinstance(nudge['body'], str) or not nudge['body'].strip():\n                    validation_result['errors'].append(f'Item {i} has invalid body')\n\n                # Check for reasonable length\n                if 'title' in nudge and len(nudge['title']) > 100:\n                    validation_result['warnings'].append(f'Item {i} title is very long ({len(nudge[\"title\"])} chars)')\n\n                if 'body' in nudge and len(nudge['body']) > 300:\n                    validation_result['warnings'].append(f'Item {i} body is very long ({len(nudge[\"body\"])} chars)')\n\n            # If no errors, mark as valid\n            if not validation_result['errors']:\n                validation_result['is_valid'] = True\n\n        except json.JSONDecodeError as e:\n            validation_result['errors'].append(f'Invalid JSON: {str(e)}')\n        except Exception as e:\n            validation_result['errors'].append(f'Validation error: {str(e)}')\n\n        return validation_result\n\n    def simulate_step_count_aggregation(self, recent_step_data: List[int]) -> int:\n        \"\"\"Simulate step count aggregation from HealthObservations_HKQuantityTypeIdentifierStepCount\"\"\"\n        if not recent_step_data:\n            return 0\n        return round(sum(recent_step_data) / len(recent_step_data))\n\n    def generate_personalized_prompt(self, language: str = 'en', recent_step_average: int = None, \n                                   education_level: str = None, user_id: str = \"test_user\") -> str:\n        \"\"\"Generate personalized prompt with user step count and education level (matching planNudges.ts structure)\"\"\"\n        \n        # Build personalization context (matching planNudges.ts:208-228)\n        personalization_context = []\n        if recent_step_average is not None:\n            personalization_context.append(\n                f\"Promedio de pasos diarios recientes: {recent_step_average}\" if language == 'es' \n                else f\"Recent daily step count average: {recent_step_average}\"\n            )\n        if education_level:\n            personalization_context.append(\n                f\"Nivel educativo: {education_level}\" if language == 'es'\n                else f\"Education level: {education_level}\"\n            )\n        \n        context_str = \"\"\n        if personalization_context:\n            if language == 'es':\n                context_str = f\"\\\\n\\\\nInformaci√≥n del participante:\\\\n{chr(10).join([f'- {ctx}' for ctx in personalization_context])}\\\\n\"\n            else:\n                context_str = f\"\\\\n\\\\nParticipant information:\\\\n{chr(10).join([f'- {ctx}' for ctx in personalization_context])}\\\\n\"\n        \n        if language == 'es':\n            # Spanish prompt matching planNudges.ts:232-249\n            base_prompt = f\"\"\"Genera 7 recordatorios motivacionales de deportes y ejercicio para un participante en un estudio de salud card√≠aca.{context_str}\nCada recordatorio debe:\n- Ser alentador y positivo\n- Enfocarse en diferentes tipos de actividades f√≠sicas y deportes\n- Ser personalizado y atractivo basado en la informaci√≥n del participante\n- Incluir una llamada clara a la acci√≥n\n- Ser adecuado para alguien en un estudio de salud card√≠aca\n- Adaptar el lenguaje y las sugerencias al nivel educativo del participante\n- Incorporar referencias al conteo de pasos cuando sea relevante\n\nDevuelve la respuesta como un array JSON con exactamente 7 objetos, cada uno con campos \"title\" y \"body\".\nFormato de ejemplo:\n[\n  {{\"title\": \"Impulso de Energ√≠a Matutino\", \"body\": \"¬°Comienza tu d√≠a con una caminata de 15 minutos! Tu coraz√≥n amar√° el cardio suave.\"}},\n  ...\n]\n\nHaz cada recordatorio √∫nico y enf√≥cate en diferentes actividades como caminar, nadar, bailar, deportes de equipo, entrenamiento de fuerza, yoga, etc.\"\"\"\n        else:\n            # English prompt matching planNudges.ts:250-267\n            base_prompt = f\"\"\"Generate 7 motivational sports and exercise nudges for a heart health study participant.{context_str}\nEach nudge should:\n- Be encouraging and positive\n- Focus on different types of physical activities and sports\n- Be personalized and engaging based on the participant's information\n- Include a clear call to action\n- Be suitable for someone in a heart health study\n- Adapt language and suggestions to the participant's education level\n- Incorporate step count references when relevant\n\nReturn the response as a JSON array with exactly 7 objects, each having \"title\" and \"body\" fields.\nExample format:\n[\n  {{\"title\": \"Morning Energy Boost\", \"body\": \"Start your day with a 15-minute walk! Your heart will love the gentle cardio.\"}},\n  ...\n]\n\nMake each nudge unique and focus on different activities like walking, swimming, dancing, team sports, strength training, yoga, etc.\"\"\"\n        \n        return base_prompt\n\n    async def test_prompt(self, prompt: str, expected_count: int = 7, description: str = \"\") -> Dict[str, Any]:\n        \"\"\"Test a prompt and validate the response\"\"\"\n        print(f\"\\\\nüß™ Testing prompt: {description or 'Unnamed test'}\")\n        print(f\"üìù Prompt length: {len(prompt)} characters\")\n\n        # Call API\n        api_result = await self.call_openai_api(prompt)\n\n        result = {\n            'timestamp': datetime.now().isoformat(),\n            'description': description,\n            'prompt': prompt,\n            'api_success': api_result['success'],\n            'api_attempts': api_result['attempt'],\n            'api_error': api_result['error'],\n            'raw_response': api_result['content'],\n            'validation': None\n        }\n\n        if api_result['success']:\n            print(f\"‚úÖ API call successful (attempt {api_result['attempt']})\")\n\n            # Validate response\n            validation = self.validate_nudge_response(api_result['content'], expected_count)\n            result['validation'] = validation\n\n            if validation['is_valid']:\n                print(f\"‚úÖ Response validation passed\")\n                if validation['warnings']:\n                    print(f\"‚ö†Ô∏è  Warnings: {len(validation['warnings'])}\")\n                    for warning in validation['warnings']:\n                        print(f\"   - {warning}\")\n            else:\n                print(f\"‚ùå Response validation failed\")\n                print(f\"üîç Errors: {len(validation['errors'])}\")\n                for error in validation['errors']:\n                    print(f\"   - {error}\")\n        else:\n            print(f\"‚ùå API call failed after {api_result['attempt']} attempts\")\n            print(f\"üîç Error: {api_result['error']}\")\n\n        # Store result\n        self.results_history.append(result)\n\n        return result\n\n    async def test_personalized_prompt(self, language: str = 'en', recent_step_average: int = None, \n                                     education_level: str = None, user_id: str = \"test_user\") -> Dict[str, Any]:\n        \"\"\"Test personalized prompt with user data (matching real Firebase structure)\"\"\"\n        prompt = self.generate_personalized_prompt(language, recent_step_average, education_level, user_id)\n        \n        description = f\"Personalized {language} prompt\"\n        if recent_step_average is not None:\n            description += f\" (avg steps: {recent_step_average})\"\n        if education_level:\n            description += f\" (edu: {education_level})\"\n            \n        return await self.test_prompt(prompt, description=description)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eistMGqg83L4"
   },
   "source": [
    "## Predefined Test Prompts\n",
    "\n",
    "Based on the nudge generation prompts from planNudges.ts:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7dqpu8VU83L4"
   },
   "outputs": [],
   "source": "# Personalized prompt examples with real user data structure\n# These match the actual Firebase user document structure\n\n# Simulate realistic step count data from HealthObservations_HKQuantityTypeIdentifierStepCount\nSAMPLE_STEP_DATA = {\n    'low_activity': [1200, 1500, 800, 2100, 1800, 1400, 1600],  # avg: 1486\n    'moderate_activity': [4500, 5200, 3800, 6100, 4900, 5300, 4700],  # avg: 4929\n    'high_activity': [8200, 9100, 7800, 8900, 8500, 8700, 9200],  # avg: 8629\n    'very_low_activity': [300, 500, 200, 800, 400, 600, 350]  # avg: 450\n}\n\n# Real educationLevel values (use exactly as stored in user documents)\nEDUCATION_LEVELS = [\n    \"High school diploma or GED\",\n    \"Some college, no degree\",\n    \"Associate degree\",\n    \"Bachelor's degree\",\n    \"Master's degree\", \n    \"Doctoral or professional degree\",\n    \"Trade/technical/vocational training\",\n    \"Less than high school\"\n]\n\n# Test profiles using real Firebase structure\nPERSONALIZED_EXAMPLES = {\n    'low_steps_graduate': {\n        'recent_step_data': SAMPLE_STEP_DATA['low_activity'],\n        'education_level': \"Master's degree\",\n        'description': 'Low activity participant with graduate education'\n    },\n    'high_steps_highschool': {\n        'recent_step_data': SAMPLE_STEP_DATA['high_activity'],\n        'education_level': \"High school diploma or GED\",\n        'description': 'High activity participant with high school education'\n    },\n    'moderate_steps_bachelor': {\n        'recent_step_data': SAMPLE_STEP_DATA['moderate_activity'],\n        'education_level': \"Bachelor's degree\",\n        'description': 'Moderate activity participant with bachelor education'\n    },\n    'very_low_steps_trade': {\n        'recent_step_data': SAMPLE_STEP_DATA['very_low_activity'],\n        'education_level': \"Trade/technical/vocational training\",\n        'description': 'Very low activity participant with technical education'\n    },\n    'moderate_steps_some_college': {\n        'recent_step_data': SAMPLE_STEP_DATA['moderate_activity'],\n        'education_level': \"Some college, no degree\",\n        'description': 'Moderate activity participant with some college'\n    }\n}\n\n# Original prompts from planNudges.ts (for comparison)\nORIGINAL_ENGLISH_PROMPT = \"\"\"Generate 7 motivational sports and exercise nudges for a heart health study participant. Each nudge should:\n- Be encouraging and positive\n- Focus on different types of physical activities and sports\n- Be personalized and engaging\n- Include a clear call to action\n- Be suitable for someone in a heart health study\n\nReturn the response as a JSON array with exactly 7 objects, each having \"title\" and \"body\" fields.\nExample format:\n[\n  {\"title\": \"Morning Energy Boost\", \"body\": \"Start your day with a 15-minute walk! Your heart will love the gentle cardio.\"},\n  ...\n]\n\nMake each nudge unique and focus on different activities like walking, swimming, dancing, team sports, strength training, yoga, etc.\"\"\"\n\nORIGINAL_SPANISH_PROMPT = \"\"\"Genera 7 recordatorios motivacionales de deportes y ejercicio para un participante en un estudio de salud card√≠aca. Cada recordatorio debe:\n- Ser alentador y positivo\n- Enfocarse en diferentes tipos de actividades f√≠sicas y deportes\n- Ser personalizado y atractivo\n- Incluir una llamada clara a la acci√≥n\n- Ser adecuado para alguien en un estudio de salud card√≠aca\n\nDevuelve la respuesta como un array JSON con exactamente 7 objetos, cada uno con campos \"title\" y \"body\".\nFormato de ejemplo:\n[\n  {\"title\": \"Impulso de Energ√≠a Matutino\", \"body\": \"¬°Comienza tu d√≠a con una caminata de 15 minutos! Tu coraz√≥n amar√° el cardio suave.\"},\n  ...\n]\n\nHaz cada recordatorio √∫nico y enf√≥cate en diferentes actividades como caminar, nadar, bailar, deportes de equipo, entrenamiento de fuerza, yoga, etc.\"\"\"\n\n# Test prompts for experimentation\nTEST_PROMPTS = {\n    'original_english': ORIGINAL_ENGLISH_PROMPT,\n    'original_spanish': ORIGINAL_SPANISH_PROMPT,\n\n    'simplified_english': \"\"\"Create 7 exercise reminders for heart health study participants. Each should have a \"title\" and \"body\". Return as JSON array.\nMake them motivational and focus on different activities.\"\"\",\n\n    'detailed_english': \"\"\"Generate exactly 7 motivational exercise nudges for heart health study participants. Requirements:\n1. Each nudge must be encouraging and positive\n2. Focus on diverse physical activities (walking, swimming, cycling, dancing, strength training, yoga, team sports)\n3. Include specific, actionable advice\n4. Appropriate for cardiovascular health improvement\n5. Vary the tone and approach for each nudge\n\nCRITICAL: Return ONLY a valid JSON array with exactly 7 objects. Each object must have exactly these fields:\n- \"title\": string (maximum 50 characters)\n- \"body\": string (maximum 150 characters)\n\nExample format:\n[{\"title\": \"Morning Walk Challenge\", \"body\": \"Start today with a 10-minute walk. Your heart will thank you!\"}]\"\"\",\n\n    'json_focused': \"\"\"You are a JSON generator. Generate exactly 7 exercise nudges.\n\nOutput ONLY valid JSON in this exact format:\n[{\"title\": \"Exercise Title\", \"body\": \"Exercise description with call to action\"}]\n\nRequirements:\n- Exactly 7 items\n- Heart health focus\n- Different activities each\n- Positive tone\n- No additional text outside JSON\"\"\"\n}\n\nprint(\"üìä Sample data loaded:\")\nprint(f\"Education levels: {len(EDUCATION_LEVELS)} options\")\nprint(f\"User profiles: {len(PERSONALIZED_EXAMPLES)} profiles\")\nprint(f\"Step data samples: {list(SAMPLE_STEP_DATA.keys())}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vi2ndgo_83L4"
   },
   "source": [
    "## Initialize Tester and Run Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pixIaqy983L4",
    "outputId": "a487f4d3-44ec-4be9-c561-07c54f836ef7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üöÄ LLM Prompt Tester initialized\n",
      "üìä Model: gpt-3.5-turbo\n",
      "üå°Ô∏è  Temperature: 0.7\n",
      "üìù Max tokens: 1000\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tester\n",
    "tester = LLMPromptTester(OPENAI_API_KEY)\n",
    "\n",
    "print(\"üöÄ LLM Prompt Tester initialized\")\n",
    "print(f\"üìä Model: {MODEL}\")\n",
    "print(f\"üå°Ô∏è  Temperature: {TEMPERATURE}\")\n",
    "print(f\"üìù Max tokens: {MAX_TOKENS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GG9YdIv_83L4"
   },
   "source": [
    "## Test Individual Prompts\n",
    "\n",
    "Run this cell to test a specific prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "ZJUl6Ih983L4",
    "outputId": "9caf5783-46f2-4e33-d127-fd675af7cd6f"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'TEST_PROMPTS' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-14-3241922099.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprompt_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ORIGINAL_ENGLISH_PROMPT'\u001b[0m  \u001b[0;31m# Change this to test different prompts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m result = await tester.test_prompt(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mTEST_PROMPTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Testing {prompt_name} prompt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TEST_PROMPTS' is not defined"
     ]
    }
   ],
   "source": [
    "# Test a specific prompt\n",
    "prompt_name = 'ORIGINAL_ENGLISH_PROMPT'  # Change this to test different prompts\n",
    "result = await tester.test_prompt(\n",
    "    TEST_PROMPTS[prompt_name],\n",
    "    description=f\"Testing {prompt_name} prompt\"\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "if result['api_success'] and result['validation']['is_valid']:\n",
    "    print(\"\\nüìã Generated Nudges:\")\n",
    "    for i, nudge in enumerate(result['validation']['parsed_data'], 1):\n",
    "        print(f\"{i}. {nudge['title']}\")\n",
    "        print(f\"   {nudge['body']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lm32i5Wj83L4"
   },
   "source": [
    "## Batch Test All Prompts\n",
    "\n",
    "Run all test prompts and compare results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhOKM7WR83L4",
    "outputId": "6fc70657-714d-43e4-f082-7ed70e083e61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Running batch tests on all prompts...\n",
      "\n",
      "\n",
      "üß™ Testing prompt: Batch test: original_english\n",
      "üìù Prompt length: 698 characters\n",
      "‚úÖ API call successful (attempt 1)\n",
      "‚úÖ Response validation passed\n",
      "\n",
      "üß™ Testing prompt: Batch test: original_spanish\n",
      "üìù Prompt length: 790 characters\n",
      "‚úÖ API call successful (attempt 1)\n",
      "‚úÖ Response validation passed\n",
      "\n",
      "üß™ Testing prompt: Batch test: simplified_english\n",
      "üìù Prompt length: 183 characters\n",
      "‚úÖ API call successful (attempt 1)\n",
      "‚úÖ Response validation passed\n",
      "\n",
      "üß™ Testing prompt: Batch test: detailed_english\n",
      "üìù Prompt length: 720 characters\n",
      "‚úÖ API call successful (attempt 1)\n",
      "‚úÖ Response validation passed\n",
      "\n",
      "üß™ Testing prompt: Batch test: json_focused\n",
      "üìù Prompt length: 321 characters\n",
      "‚úÖ API call successful (attempt 1)\n",
      "‚úÖ Response validation passed\n",
      "\n",
      "‚úÖ Batch testing complete!\n"
     ]
    }
   ],
   "source": [
    "# Test all prompts\n",
    "print(\"üî¨ Running batch tests on all prompts...\\n\")\n",
    "\n",
    "batch_results = {}\n",
    "for prompt_name, prompt_text in TEST_PROMPTS.items():\n",
    "    result = await tester.test_prompt(\n",
    "        prompt_text,\n",
    "        description=f\"Batch test: {prompt_name}\"\n",
    "    )\n",
    "    batch_results[prompt_name] = result\n",
    "\n",
    "    # Small delay to avoid rate limiting\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\n‚úÖ Batch testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WyvV5k783L5"
   },
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIhkDSOF83L5",
    "outputId": "3479a3fc-13ca-4f6d-f21f-ddadd7e2654a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Results Summary:\n",
      "       prompt_name  api_success  api_attempts  validation_success  error_count  warning_count  prompt_length\n",
      "  original_english         True             1                True            0              0            698\n",
      "  original_spanish         True             1                True            0              0            790\n",
      "simplified_english         True             1                True            0              0            183\n",
      "  detailed_english         True             1                True            0              0            720\n",
      "      json_focused         True             1                True            0              0            321\n",
      "\n",
      "üéØ Overall success rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Analyze results\n",
    "results_summary = []\n",
    "\n",
    "for prompt_name, result in batch_results.items():\n",
    "    summary = {\n",
    "        'prompt_name': prompt_name,\n",
    "        'api_success': result['api_success'],\n",
    "        'api_attempts': result['api_attempts'],\n",
    "        'validation_success': result['validation']['is_valid'] if result['validation'] else False,\n",
    "        'error_count': len(result['validation']['errors']) if result['validation'] else 0,\n",
    "        'warning_count': len(result['validation']['warnings']) if result['validation'] else 0,\n",
    "        'prompt_length': len(result['prompt'])\n",
    "    }\n",
    "    results_summary.append(summary)\n",
    "\n",
    "# Create results DataFrame\n",
    "df_results = pd.DataFrame(results_summary)\n",
    "print(\"üìä Results Summary:\")\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# Success rate\n",
    "success_rate = (df_results['validation_success'].sum() / len(df_results)) * 100\n",
    "print(f\"\\nüéØ Overall success rate: {success_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJLGJ9iE83L5"
   },
   "source": "## Personalized Prompt Testing\n\nTest personalized prompts with different user profiles:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u0eqwhpF83L5",
    "outputId": "a8248fc3-a372-4857-a23b-0723d925bd5c"
   },
   "outputs": [],
   "source": "# Test personalized prompts with real Firebase user structure\nprofile_name = 'low_steps_graduate'  # Change this to test different profiles\nprofile = PERSONALIZED_EXAMPLES[profile_name]\n\n# Simulate step count aggregation (matching planNudges.ts:184)\nrecent_step_average = tester.simulate_step_count_aggregation(profile['recent_step_data'])\n\nprint(f\"üéØ Testing profile: {profile['description']}\")\nprint(f\"üìä Recent step data: {profile['recent_step_data']}\")\nprint(f\"üìà Average daily steps: {recent_step_average}\")\nprint(f\"üéì Education level: {profile['education_level']}\")\n\n# Test English personalized prompt\nresult_en = await tester.test_personalized_prompt(\n    language='en',\n    recent_step_average=recent_step_average,\n    education_level=profile['education_level'],\n    user_id=f\"test_{profile_name}\"\n)\n\n# Display results\nif result_en['api_success'] and result_en['validation']['is_valid']:\n    print(\"\\nüìã Personalized English Nudges:\")\n    for i, nudge in enumerate(result_en['validation']['parsed_data'], 1):\n        print(f\"{i}. {nudge['title']}\")\n        print(f\"   {nudge['body']}\\n\")\n\n# Test Spanish personalized prompt\nprint(\"\\n\" + \"=\"*50)\nresult_es = await tester.test_personalized_prompt(\n    language='es',\n    recent_step_average=recent_step_average,\n    education_level=profile['education_level'],\n    user_id=f\"test_{profile_name}_es\"\n)\n\n# Display results\nif result_es['api_success'] and result_es['validation']['is_valid']:\n    print(\"\\nüìã Personalized Spanish Nudges:\")\n    for i, nudge in enumerate(result_es['validation']['parsed_data'], 1):\n        print(f\"{i}. {nudge['title']}\")\n        print(f\"   {nudge['body']}\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJ1Kc5bY83L5"
   },
   "source": "## Batch Test All Personalized Profiles\n\nTest all user profiles to compare personalization effectiveness:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "65TpovEp83L5"
   },
   "outputs": [],
   "source": "# Test all personalized profiles with real Firebase data structure\nprint(\"üî¨ Running batch tests on all personalized profiles...\\n\")\n\npersonalized_results = {}\nfor profile_name, profile_data in PERSONALIZED_EXAMPLES.items():\n    print(f\"Testing profile: {profile_data['description']}\")\n    \n    # Simulate step count aggregation (matching planNudges.ts getRecentStepCount method)\n    recent_step_average = tester.simulate_step_count_aggregation(profile_data['recent_step_data'])\n    \n    # Test English\n    result_en = await tester.test_personalized_prompt(\n        language='en',\n        recent_step_average=recent_step_average,\n        education_level=profile_data['education_level'],\n        user_id=f\"test_{profile_name}_en\"\n    )\n    \n    # Test Spanish\n    result_es = await tester.test_personalized_prompt(\n        language='es',\n        recent_step_average=recent_step_average,\n        education_level=profile_data['education_level'],\n        user_id=f\"test_{profile_name}_es\"\n    )\n    \n    personalized_results[f\"{profile_name}_en\"] = result_en\n    personalized_results[f\"{profile_name}_es\"] = result_es\n    \n    # Log the step aggregation for verification\n    print(f\"  - Step data: {profile_data['recent_step_data']}\")\n    print(f\"  - Calculated average: {recent_step_average} steps/day\")\n    print(f\"  - Education: {profile_data['education_level']}\")\n    \n    # Small delay to avoid rate limiting\n    import time\n    time.sleep(2)\n\nprint(\"\\n‚úÖ Personalized batch testing complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVUCSMkO83L5"
   },
   "source": "## Personalization Analysis\n\nAnalyze how personalization affects the generated nudges:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sj4zqLsE83L5"
   },
   "outputs": [],
   "source": "# Analyze personalization effectiveness\ndef analyze_personalization(results: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Analyze how personalization affects nudge content\"\"\"\n    analysis = {\n        'step_references': 0,\n        'education_adaptations': 0,\n        'complexity_scores': [],\n        'activity_mentions': {},\n        'personalization_indicators': []\n    }\n    \n    for result_name, result in results.items():\n        if not (result['api_success'] and result['validation'] and result['validation']['is_valid']):\n            continue\n            \n        nudges = result['validation']['parsed_data']\n        \n        for nudge in nudges:\n            title = nudge.get('title', '').lower()\n            body = nudge.get('body', '').lower()\n            content = f\"{title} {body}\"\n            \n            # Check for step references\n            if any(word in content for word in ['step', 'walk', 'pace', 'distance']):\n                analysis['step_references'] += 1\n            \n            # Check for complexity indicators\n            complex_words = ['cardiovascular', 'metabolism', 'endurance', 'intensity', 'optimize']\n            simple_words = ['move', 'fun', 'easy', 'try', 'start']\n            \n            complexity_score = sum(1 for word in complex_words if word in content)\n            complexity_score -= sum(1 for word in simple_words if word in content) * 0.5\n            analysis['complexity_scores'].append(complexity_score)\n            \n            # Track activity mentions\n            activities = ['walk', 'run', 'swim', 'dance', 'yoga', 'bike', 'gym', 'sport']\n            for activity in activities:\n                if activity in content:\n                    analysis['activity_mentions'][activity] = analysis['activity_mentions'].get(activity, 0) + 1\n    \n    # Calculate averages\n    if analysis['complexity_scores']:\n        analysis['avg_complexity'] = sum(analysis['complexity_scores']) / len(analysis['complexity_scores'])\n    \n    return analysis\n\n# Analyze personalized results\nif 'personalized_results' in locals():\n    personalization_analysis = analyze_personalization(personalized_results)\n    \n    print(\"üìà Personalization Analysis:\")\n    print(f\"Step references found: {personalization_analysis['step_references']}\")\n    print(f\"Average complexity score: {personalization_analysis.get('avg_complexity', 0):.2f}\")\n    print(f\"Most mentioned activities: {sorted(personalization_analysis['activity_mentions'].items(), key=lambda x: x[1], reverse=True)[:3]}\")\n    \n    # Create comparison table\n    comparison_data = []\n    for result_name, result in personalized_results.items():\n        if result['api_success'] and result['validation'] and result['validation']['is_valid']:\n            profile_parts = result_name.split('_')\n            language = profile_parts[-1]\n            profile_type = '_'.join(profile_parts[:-1])\n            \n            comparison_data.append({\n                'profile': profile_type,\n                'language': language,\n                'success': result['validation']['is_valid'],\n                'prompt_length': len(result['prompt']),\n                'description': result['description']\n            })\n    \n    if comparison_data:\n        df_personalized = pd.DataFrame(comparison_data)\n        print(\"\\nüìä Personalized Results Summary:\")\n        print(df_personalized.to_string(index=False))\nelse:\n    print(\"No personalized results to analyze. Run the batch test first!\")"
  },
  {
   "cell_type": "code",
   "source": "# Custom personalized testing\ncustom_step_count = 3000  # Modify these values to test different user profiles\ncustom_education = \"College degree\"  # Options: \"High school\", \"College degree\", \"Graduate degree\", \"Technical/vocational training\"\ncustom_language = \"en\"  # \"en\" or \"es\"\n\nprint(f\"üéØ Testing custom profile:\")\nprint(f\"üìä Step count: {custom_step_count}\")\nprint(f\"üéì Education level: {custom_education}\")\nprint(f\"üåê Language: {custom_language}\")\n\n# Generate and display the personalized prompt\ncustom_prompt = tester.generate_personalized_prompt(\n    language=custom_language,\n    step_count=custom_step_count,\n    education_level=custom_education\n)\n\nprint(f\"\\nüìù Generated prompt preview (first 300 chars):\")\nprint(custom_prompt[:300] + \"...\" if len(custom_prompt) > 300 else custom_prompt)\n\n# Test the custom personalized prompt\ncustom_result = await tester.test_personalized_prompt(\n    language=custom_language,\n    step_count=custom_step_count,\n    education_level=custom_education,\n    user_id=\"custom_test_user\"\n)\n\n# Display results\nif custom_result['api_success'] and custom_result['validation']['is_valid']:\n    print(f\"\\nüìã Custom Personalized Nudges ({custom_language.upper()}):\")\n    for i, nudge in enumerate(custom_result['validation']['parsed_data'], 1):\n        print(f\"{i}. {nudge['title']}\")\n        print(f\"   {nudge['body']}\\n\")\n        \n    # Analyze this specific result for personalization indicators\n    nudges = custom_result['validation']['parsed_data']\n    step_mentions = sum(1 for nudge in nudges \n                       if any(word in f\"{nudge.get('title', '')} {nudge.get('body', '')}\".lower() \n                             for word in ['step', 'walk', 'pace', 'move']))\n    \n    print(f\"üìà Personalization analysis:\")\n    print(f\"   - Step/movement references: {step_mentions}/7 nudges\")\n    print(f\"   - Average title length: {sum(len(n.get('title', '')) for n in nudges) / len(nudges):.1f} chars\")\n    print(f\"   - Average body length: {sum(len(n.get('body', '')) for n in nudges) / len(nudges):.1f} chars\")\nelse:\n    print(\"‚ùå Custom test failed. Check the error messages above.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Custom personalized testing with real Firebase structure\n# Modify these values to test different user profiles\n\n# Sample step count data (7 days as collected from HealthObservations_HKQuantityTypeIdentifierStepCount)\ncustom_step_data = [3000, 3500, 2800, 4200, 3800, 3200, 3100]  # Sample 7-day step data\ncustom_education = \"Bachelor's degree\"  # Use exact values from EDUCATION_LEVELS\ncustom_language = \"en\"  # \"en\" or \"es\"\n\n# Calculate average (matching planNudges.ts aggregation logic)\ncustom_step_average = tester.simulate_step_count_aggregation(custom_step_data)\n\nprint(f\"üéØ Testing custom profile:\")\nprint(f\"üìä Raw step data (7 days): {custom_step_data}\")\nprint(f\"üìà Calculated average: {custom_step_average} steps/day\")\nprint(f\"üéì Education level: {custom_education}\")\nprint(f\"üåê Language: {custom_language}\")\n\n# Verify education level is valid\nif custom_education not in EDUCATION_LEVELS:\n    print(f\"‚ö†Ô∏è  Warning: '{custom_education}' not in standard education levels\")\n    print(f\"Valid options: {EDUCATION_LEVELS}\")\n\n# Generate and display the personalized prompt\ncustom_prompt = tester.generate_personalized_prompt(\n    language=custom_language,\n    recent_step_average=custom_step_average,\n    education_level=custom_education\n)\n\nprint(f\"\\nüìù Generated prompt preview (first 300 chars):\")\nprint(custom_prompt[:300] + \"...\" if len(custom_prompt) > 300 else custom_prompt)\n\n# Test the custom personalized prompt\ncustom_result = await tester.test_personalized_prompt(\n    language=custom_language,\n    recent_step_average=custom_step_average,\n    education_level=custom_education,\n    user_id=\"custom_test_user\"\n)\n\n# Display results\nif custom_result['api_success'] and custom_result['validation']['is_valid']:\n    print(f\"\\nüìã Custom Personalized Nudges ({custom_language.upper()}):\")\n    for i, nudge in enumerate(custom_result['validation']['parsed_data'], 1):\n        print(f\"{i}. {nudge['title']}\")\n        print(f\"   {nudge['body']}\\n\")\n        \n    # Analyze this specific result for personalization indicators\n    nudges = custom_result['validation']['parsed_data']\n    step_mentions = sum(1 for nudge in nudges \n                       if any(word in f\"{nudge.get('title', '')} {nudge.get('body', '')}\".lower() \n                             for word in ['step', 'walk', 'pace', 'move', 'distance']))\n    \n    # Check for education-appropriate language complexity\n    complex_words = ['cardiovascular', 'metabolism', 'endurance', 'optimize', 'intensity']\n    simple_words = ['move', 'fun', 'easy', 'try', 'start', 'great']\n    \n    complex_count = sum(1 for nudge in nudges\n                       for word in complex_words\n                       if word.lower() in f\"{nudge.get('title', '')} {nudge.get('body', '')}\".lower())\n    \n    simple_count = sum(1 for nudge in nudges  \n                      for word in simple_words\n                      if word.lower() in f\"{nudge.get('title', '')} {nudge.get('body', '')}\".lower())\n    \n    print(f\"üìà Personalization analysis:\")\n    print(f\"   - Step/movement references: {step_mentions}/7 nudges\")\n    print(f\"   - Complex vocabulary usage: {complex_count} instances\")\n    print(f\"   - Simple vocabulary usage: {simple_count} instances\")\n    print(f\"   - Average title length: {sum(len(n.get('title', '')) for n in nudges) / len(nudges):.1f} chars\")\n    print(f\"   - Average body length: {sum(len(n.get('body', '')) for n in nudges) / len(nudges):.1f} chars\")\n    print(f\"   - Data used: {custom_step_average} avg steps, '{custom_education}' education\")\nelse:\n    print(\"‚ùå Custom test failed. Check the error messages above.\")",
   "metadata": {},
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}